---
layout: default
encoding: "utf-8"
---


<div class="container-fluid index">
    <div class="row">

          <div class="col-md-12 main content-panel">

            <div class="articles">

              <h2>15-418 Final Project Report</h2>

              <h3>Bromide -- Fast CNN Inference In Halide</h3>
              <br/>
              <a class="links-sm-418" href="{{ site.baseurl }}/15418">back to project index</a>
              <br/>
              <br/>
              <a class="links-sm-418" href="https://github.com/vanryan/bromide">github</a>
              <br/>
              <br/>


<!--&&&&&& divider &&&&&&-->
<h4>Summary</h4>
<p class="parag-418">
We have implemented a general <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> inference program using <a href="http://halide-lang.org/">the Halide image processing language</a> which is scheduled to be <span class="imp">fast(-er than Caffe, up to 3x)</span> on a single machine using CPU.
</p>
<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>Why we did this (Background)</h4>
<br/>
<h5><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a></h5>
<p class="parag-418">
Lots of modern-day machine learning applications, such as AlphaGo, are using Convolutional Neural Networks (CNNs) for classification tasks. Therefore, the performance of training and inference with CNNs is fairly important. For huge data sets and large CNN configurations, training can take several days, making it meaningful to speed up the training process. Besides, in real-time applications, such as Apple Siri, the time spent in inference is also expected to be short, so that the delay will be tolerable or more satisfactory for users.
</p>

<p class="parag-418">For testing purpose, we are using <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> (a database of handwritten digits and we use a typical CNN, <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a> to perform inference on it) and <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>(which is designed to do classification towards <a href="http://image-net.org/">ImageNet</a>). 

<p class="parag-418">
The structure of <span class="imp">LeNet</span> is illustrated as below:</p>
<img src="{{ site.baseurl }}/resources/lenet.png" alt="lenet" height="152" width="600" /> 
<p class="parag-418"> The LeNet that we use is mainly composed of the following layers by order: <i>conv, pooling, conv, pooling, fully-connected, relu, fully-connected, softmax.</i></p>


<p class="parag-418">
The structure of <span class="imp">AlexNet</span> is illustrated as below:</p>

<img src="{{ site.baseurl }}/resources/alexnet.png" alt="alexnet" height="252" width="600" /> 
<p class="parag-418"> The AlexNet that we use is mainly composed of the following layers by order: <i>conv, relu, norm, pooling, conv, relu, norm, pooling, conv, relu, conv, relu, conv, relu, pooling, fully-connected, relu, drop, fully-connected, relu, drop, fully-connected, softmax.</i></p>

<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h5>Caffe</h5>
<p class="parag-418">
<a href="http://caffe.berkeleyvision.org/">Caffe</a> is a deep learning framework currently widely used to both train and test CNN. It is known for its speed, modularity and clear expressions and settings of networks and relevant processes. In our project, Caffe is selected to be a benchmark for us to compete against in terms of speed.
</p>

<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h5><a href="http://halide-lang.org/">Halide</a></h5>
<p class="parag-418">
Halide is a language and compiler for optimizing parallelism,locality, and re-computation in image processing pipelines. Since it provides many scheduling interfaces to handle matrices, it is a great source for CNN implementation, where the intermediate results are also matrices. Therefore, by carefully scheduling CNN training and inference with Halide, an obvious speedup is expected.
</p>

<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>What we did (Results)</h4>
<p class="parag-418">
We have implemented this inference program which is tested towards MNIST (LeNet) and <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet"> AlexNet (ImageNet)</a>, and it is compared against <a href="https://github.com/BVLC/caffe">Caffe</a> (testing of the network). Luckily, we have achieved better performance than Caffe in our limited tested scenarios. The following figures and data generally use Caffe as their counterpart for comparison. Our implementation is referred to as <span class="brom">"bromide"</span>.
</p>

<p class="parag-418">
The <span class="imp">throughput</span> of LeNet is illustrated as below:</p>
<img src="{{ site.baseurl }}/resources/lenet_throughput.jpg" alt="lenet" height="462" width="600" /> 
<br/><br/>

<p class="parag-418">
The <span class="imp">throughput</span> of AlexNet is illustrated as below:</p>
<img src="{{ site.baseurl }}/resources/alexnet_throughput.jpg" alt="alexnet" height="462" width="600" /> 
<br/><br/>

<p class="parag-418">
The <span class="imp">latency</span> of LeNet and AlexNet is illustrated as below: (<span class="imp">latency</span> here stands for the time to perform inference on a <span class="imp2">single</span> input image, the smaller the better)
</p>
<img src="{{ site.baseurl }}/resources/latency.jpg" alt="latency" height="452" width="600" /> 
<br/><br/>

<p class="parag-418">
As we can see from the figures above, for the batchsize 16 of LeNet, the speedup (in terms of throughput) of Bromide over Caffe is roughly <span class="imp">3.04x</span>, and the average latency is <span class="imp">21% (LeNet) and 26% (AlexNet) better</span> than Caffe. Although these are not very significant, the results surely give us some hope in the power of scheduling in Halide.
</p>

<div class="par-blankline-10"></div>
<h5>Experimental setup:</h5>
<p class="parag-418">
<span class="imp2">Input (LeNet): </span>28x28x1 <br/>
<span class="imp2">Input (AlexNet): </span>227x227x3<br/>
<span class="imp2">Output: </span>label/classification of the picture. We actually stopped after the last layer (softmax) because choosing the most likely label is irrelevant with the performance, so this part is cut after we tested the correctness of the network)<br/>
<span class="imp2">Machine: </span>all of these tests are conducted on a machine that has 2 processors (Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz). This kind of CPU has 4 cores and 8 thereads, with 8MB cache size.<br/>
<span class="imp2">Batch size: </span> batch size stands for the number of images for one batch of input. It ranges from 1 to 256, so we have a good observation of the change of performance.<br/>
</p>

<div class="par-blankline-10"></div>
<h5>Profile of runtime:</h5>
<p class="parag-418">This is the profile for a run of batch size 64 on AlexNet (We ommitted the items with roughly 0% time):</p>
<table class="parag-418">
<tr>
  <th>Layer</th><th>Time(ms)</th><th>%</th>
</tr>
<tr>
  <td>conv</td><td><span class="imp2">436.2</span></td><td>8</td>
</tr>
<tr>
  <td>constant_exterior</td><td>375.8</td><td>7</td>
</tr>
<tr>
  <td>norm</td><td>135.9</td><td>2</td>
</tr>
<tr>
  <td>sum#1</td><td>114.9</td><td>2</td>
</tr>
<tr>
  <td>conv</td><td><span class="imp2">502.1</span></td><td>10</td>
</tr>
<tr>
  <td>constant_exterior</td><td>348.2</td><td>7</td>
</tr>
<tr>
  <td>norm</td><td>88.5</td><td>1</td>
</tr>
<tr>
  <td>sum#2</td><td>71.7</td><td>1</td>
</tr>
<tr>
  <td>conv</td><td><span class="imp2">266.3</span></td><td>5</td>
</tr>
<tr>
  <td>constant_exterior</td><td>280.0</td><td>5</td>
</tr>
<tr>
  <td>conv</td><td><span class="imp2">409.5</span></td><td>8</td>
</tr>
<tr>
  <td>constant_exterior</td><td>467.2</td><td>9</td>
</tr>
<tr>
  <td>conv</td><td><span class="imp2">342.5</span></td><td>6</td>
</tr>
<tr>
  <td>constant_exterior</td><td>284.0</td><td>5</td>
</tr>
<tr>
  <td>sum#3(fully)</td><td>492.6</td><td>9</td>
</tr>
<tr>
  <td>sum#4(fully)</td><td>100.52</td><td>2</td>
</tr>
<tr>
  <td>sum#6(fully)</td><td>207.2</td><td>4</td>
</tr>
</table>

<p class="parag-418"><i>conv: convolutional layer. norm: normalization layer. constant_exterior & sum: some halide built-in functions. The last fully-connected layers spent most of their time on sum().</i></p>

<div class="par-blankline-10"></div>
<p class="parag-418">We can tell that most time-consuming layers, as we expected, is convolutional layers and fully connected layers. constant_exterior mostly take place in convolution layers and the last 3 sum functions are in fully-connected layers. Therefore, what we can do is to further tune the convolutional layer and fully connected layer. One thing we noticed when we compared to Caffe is that, our convolutional layer outperforms Caffe a lot, while Caffe's fully-connected layer performs better than Bromide's. This is not unexpected, because there is still a lot to do in the scheduling of <span class="imp2">fully-connected </span> layer. If we can futher reduce the time there (getting rid of sum() reduction and performing more fine-tuning), the performance of bromide would be a lot more desirable.</p>



<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>How we did it (A: Methodology)</h4>

<p class="parag-418">
Usually, when we implement a CNN, we would like to build it layer by layer. One problem lying here is that every time we transmit the intermidiate results from one layer to its successor layer, we need to do a read and a write. 
</p><p class="parag-418">
This seems natural, but sometimes unnecessary. Take activation layer as an example, it's doing per-element computation, such as applying a ReLU function to all elements in the intermediate maps. Actually, this procedure can be embedded in the either the previous layer or the next layer, to avoid a read and a write. This loop-fusion method can be easily impelemented using Halide.  
</p><p class="parag-418">
Another thing worth noting is that the proper use of L3 cache can greatly increase the performance. When the data need to be reused, we can benefit from letting the data sit in the cache and wait for being called next time. Convolution layer, pooling layer, normalization layer and fully-connected layer are probably involved with data reuse. Therefore, tiling the maps can be a reasonable way. 
</p><p class="parag-418">
Also, we made an effort to make good use of SIMD and multi-threading techniques, based on their different characteristics. When using SIMD, we hope there's less divergence. Fortunately, most of the computations in CNN don't include braches. Therefore, we can use SIMD either in the inner loop or the outer loop. Since multi-threading and scheduling were to take place, we performed vectorization in the inner loop. When using multi-threading, we hope there's less synchronization. We chose to do multi-threading on the outermost loop(s) in Bromide and it turns out to be a good choice. Also, we note that the total number of threads in a machine is far less than the actual parallelizable units. To reduce synchronization, it's better to put multi-threading at the outer loop of the embedded loops. 
</p><p class="parag-418">
Furthermore, other scheduling approaches, such as tiling, unrolling, and reordering, are applied here, depending on the specific layers and map sizes. It took us a great amount of time to tune the scheduling for the layers (mainly convolutional layers). 
</p>

<h5>Hints from Caffe</h5>
<p class="parag-418">We actually profiled Caffe to try to get some hints to perform our optimization. This is an example of batch size 64 on AlexNet: </p>
<table class="parag-418">
<tr>
  <th>Layer</th><th>Time(ms)</th>
</tr>
<tr>
  <td>conv1</td><td><span class="imp2">671.791</span></td>
</tr>
<tr>
  <td>relu1</td><td>14.302</td>
</tr>
<tr>
  <td>norm1</td><td>529.359</td>
</tr>
<tr>
  <td>pool1</td><td>56.879</td>
</tr>
<tr>
  <td>conv2</td><td><span class="imp2">1246.95</span></td>
</tr>
<tr>
  <td>relu2</td><td>9.052</td>
</tr>
<tr>
  <td>norm2</td><td>343.957</td>
</tr>
<tr>
  <td>pool2</td><td>39.369</td>
</tr>
<tr>
  <td>conv3</td><td><span class="imp2">1022.71</span></td>
</tr>
<tr>
  <td>relu3</td><td>3.168</td>
</tr>
<tr>
  <td>conv4</td><td><span class="imp2">791.008</span></td>
</tr>
<tr>
  <td>relu4</td><td>3.296</td>
</tr>
<tr>
  <td>conv5</td><td><span class="imp2">542.841</span></td>
</tr>
<tr>
  <td>relu5</td><td>2.103</td>
</tr>
<tr>
  <td>pool5</td><td>9.796</td>
</tr>
<tr>
  <td>fully-connected6</td><td>196.115</td>
</tr>
<tr>
  <td>relu6</td><td>0.202</td>
</tr>
<tr>
  <td>drop6</td><td>1.854</td>
</tr>
<tr>
  <td>fully-connected7</td><td>86.68</td>
</tr>
<tr>
  <td>relu7</td><td>0.203</td>
</tr>
<tr>
  <td>drop7</td><td>1.857</td>
</tr>
<tr>
  <td>fully-connected8</td><td>21.642</td>
</tr>
</table>
<div class="par-blankline-10"></div>
<p class="parag-418">As we can tell from the table, most of the time are spent on convolutional layers. Caffe transformed convolution into matrix multiplication, where it can make use of well-tuned implementations like Intel's MKL to accelerate the speed. However, we think this can be improved since Caffe's method will generate a lot of memory traffic and do a lot of redundat. Therefore, performing well-tuned scheduling based on the definition of convolution might give us speedup, which is the most significant part of our implementation.</p>


<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>How we did it (B: Details)</h4>
<p class="parag-418">
We would introduce the details of the implementation (majorly scheduling) by each layer. Flatten layer, activation layer and drop layer are fairly straightforward thus we would mainly discuss other layers about their scheduling.
</p>

<h5>Basic Settings</h5>
<p class="parag-418">
Every layer has a member called <span class="imp2">cnnff</span>, standing for the collection of the feedforward values. It generally has 4 dimensions, <span class="imp2">(x, y, z, w)</span>, corresponding to the column-index, row-index, map-index (3rd dimension), and batch size. All those layers are written in separate files and they are welded in the main function to build networks for testing. Scheduling is performed inside the layers.
</p>

<p class="parag-418">
Kernels of convolution are traversed using <span class="imp2">Halide::RDom</span>, which is denoted as dom in our program and used to construct a multi-dimensional reduction domain. If the kernel has 3 dimensions, it will be denoted by dom.x, dom.y, dom.z.
</p>

<h5>Input Layer</h5>
<p class="parag-418">
We process the input data (trained weights) using a similar way to Caffe. The model (bvlc_alexnet.caffemodel) and mean (imagenet_mean.inarayproto) are parsed based on the functions provided by Caffe responsible for translating files using Protocol Buffer.
</p>

<h5>Convolutional Layer</h5>


<p class="parag-418">
Version 1:
</p>
<blockquote class="parag-418">conv_basic(x, y, z, w) = Halide::sum(last_layer.cnnff(x + dom.x, y + dom.y, dom.z + group_idx * input_group_size, w) * kernels(dom.x, dom.y, dom.z, map_idx + group_idx * group_size) + bias(z));</blockquote>
<p class="parag-418">
We are mainly doing intuitive implementation as well as using <span class="imp2">Halide::sum()</span> and <span class="imp2">Func.ompute_root()</span> in version 1. In convolutional layer, each pixel of an output map is computed according to the definition as the weighted sum of a filter window.
</p>
<p class="parag-418">
The <span class="imp2">Halide::sum()</span> function is used for inline reduction over the given domain (in this case, the domain would be 3-d, corresponding to the filter window of convolution). Func.compute_root() in Halide is used to compute the <span class="imp2">Func</span> ahead of the following program (<span class="imp2">Funcs</span> will be combined inline in Halide if there is no functions like compute_root() or compute_at() to explicitly order it to perform computation to the point). Func is a Halide class standing for a pipleline stage, which is a pure function that defines what value each pixel should have, similar to a computed image.
</p>

<p class="parag-418">
Version 2:<br/>
</p>
<blockquote class="parag-418">
input_ = Halide::BoundaryConditions::constant_exterior(last_layer.cnnff, 0, 0, last_layer.size[0], 0, last_layer.size[1]);<br/>
cnnff(x, y, z, w) = bias(z);<br/>
cnnff(x, y, z, w) += kernels(dom.x, dom.y, dom.z, z) * input_(x * stride_x + dom.x - pad_x, y * stride_y + dom.y - pad_y, dom.z, w);<br/>
cnnff.update().split(y, y_outer, y_inner, y_size).split(z, z_outer, z_inner, z_size); <br/>
cnnff.update().reorder(y_inner, z_inner, y_outer, dom.z, z_outer).vectorize(x, 8); <br/>
cnnff.update().parallel(w);<br/>
cnnff.update().unroll(dom.x).unroll(dom.y);<br/>
input_.compute_at(cnnff, z_inner);<br/>
</blockquote>

<p class="parag-418">
Convolution layer is a meaningful implementation exploiting the power of scheduling in Halide. We basically split y and z and reorder to get a tile, in order to find a point that reaches a good balance of memory traffic and locality which leads to a good performance. A typical tile would be loop over (from inner to outer) y_inner, y_outer, z_inner, z_outer, however, reordering the domain loop dom.z before (y_outer or) z_outer would led to a elevation of performance because of more locality. Vectorization, parallelism and unrolling were performed here as well. It is fairly intuitive to vectorize over the innermost variable (x) of cnnff and parallelize the outermost one. This is tested as well, we noticed a performance drop if on other layers. Unrolling is performed at two innermost domain variables (dom.x and dom.y) since they have a typically small range, which is the same reason why we did not put vectorization onto dom.x or dom.y. The <span class="imp2">compute_at(func, var)</span> function performs computation of func for every unique value of var, which would enable us to compute the block of inner loop variables (y_inner and z_inner) to actually instantiate the above scheduling and increase data locality and reduce redundant calculation. y_size and z_size are 2 parameters to tune towards the tile scheduling. They varies on different networks and different machines. On our settings, a good value would be 32 for both.
</p>

<h5>Fully Connected Layer</h5>
<blockquote class="parag-418">
Halide::RDom dom(0, last_layer.size[0]);<br/>
      cnnff(x, y, z, w) = Halide::sum(last_layer.cnnff(dom.x, y, z, w) * weights(dom.x, x)) + bias(x); <br/>
      cnnff.parallel(w);<br/>
          cnnff.vectorize(x, 8);<br/>
          cnnff.compute_root();<br/>
</blockquote>

<p class="parag-418">
Fully connected layer is basically a <span class="imp2">matrix multiplication</span>. If the batch size is equal to 1, it's a matrix multiplied by a vector. Otherwise, it's a multiplication between two matrices. A lot of the optimization/scheduling methods can be used here. Note that for the multiplication between a matrix and a vector, there's data reuse only in terms of the vector, but no data reuse for matrix. So a reasonable scheduling is row-by-row multiplication between matrix and vector. However, for the multiplication between two matrices, more sophisticated methods should be considered. In this project, we tried tiling the matrices into blocks, and tuning the block size to let it fit in the L3 cache. But we found that in our experiments, similarly good perforamnce can be achieved by the straighforward impelmentation of matrix multiplication. 
</p>
<p class="parag-418">
Also, for both matrix-vector multiplication and matrix-matrix multiplication, we make use of <span class="imp2">SIMD</span> and <span class="imp2">multi-threading</span> implementation. We schedule them in different dimensions so that they can work together with less synchronization. We use multi-threading for the "batch" dimension, referring to any test image. And we use SIMD for each image. To compute each pixel, there's a inner-product between vectors. This procedure is done by the reduction function <span class="imp">sum()</span> built in Halide.
</p>

<h5>Pooling Layer</h5>
<blockquote class="parag-418">
Halide::Func sub_maps("sub_maps");<br/>
                Halide::RDom dom(0, pool_x, 0, pool_y);<br/>
                if(pool_func == "max") {<br/>
                &nbsp;&nbsp;        sub_maps(x, y, z, w) = Halide::maximum(last_layer.cnnff(x + dom.x, y + dom.y, z, w));<br/>
                }else if(pool_func == "average") {<br/>
                &nbsp;&nbsp;        sub_maps(x, y, z, w) = Halide::sum(last_layer.cnnff(x + dom.x, y + dom.y, z, w)) / (pool_x * pool_y);<br/>
                }<br/>
                cnnff(x, y, z, w) = sub_maps(x * stride_x - pad_x, y * stride_y - pad_y, z, w);<br/>
        cnnff.vectorize(x, 8);<br/>
                cnnff.compute_root();<br/>

</blockquote>

<p class="parag-418">
Pooling layer is similar to the convolution layer, in that there is a window that traverses the maps, and performs <span class="imp2">reduction</span> at each time. Therefore, similar optimization (similar to the above) can be used in pooling.
</p>
<p class="parag-418">
But we also notice that, pooling doesn't take a large proportion in the overall latency. Therefore, we simply used the built-in reduction function <span class="imp2">max</span>, and vectorized it by SIMD. We didn't choose to use multi-threading, because the overhead is not worthwhile. In our observation, pooling layer of bromide can achieve <span class="imp">10x</span> speedup compared to caffe implementation.
</p>


<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>Further Analysis of  Implementation</h4>
<div class="par-blankline-10"></div>
<h5>General Challenges</h5>
<p class="parag-418">
Halide is a language targeted at image processing, which provides a set of computation scheduling methods to help programmers exploit data locality and achieve higher performance when writing parallel programs. However, since it is not designed for deep learning methods, it does not have intrinsic support for the typical computation patterns or algorithms in deep learning, for example, convolution.   Nowadays there are many a mature (sort of) implementation of deep learning methods. <br/><br/>
For example, Caffe, is a frequently seen framework in the world of deep neural networks. One feature about Caffe is that it takes advantage of a fast matrix multiplication library like Intel's MKL when running convolutional neural networks by using matrix multiplications. However, breaking down the convolutional layers to matrix multiplications might not produce a desirable performance speedup since Halide does not natively support matrix multiplication. We thought of using FFT could avoid both the original complex convolution computations and the need for fast matrix multiplication. Although doing the DFT and its inverse will take some time but this might be better than the matrix multiplications way. Anyway, we think doing Fourier transform only makes sense when the weight has one or more huge dimesion(s), so the target that we aim here is to schedule and tune the program using Halide to achieve admissible performance on CPU or GPU. This really turned out to be tough.   It is not easy to understand all the scheduling capabilities provided by Halide. Although the syntax and configurations are made very intuitive and powerful, the conciseness takes a lot of effort to grasp the scheduling. It took us a lot of time to understand and test in terms of making tradeoff between memory bandwidth (locality), parallelism and doing redundant work, which are made more explicit by Halide.<br/><br/> 
</p>

<h5>Analysis of Implementation</h5>
<p class="parag-418">
One limitation of our speedup is that it is difficult to make the "best" <span class="imp2">trade-off</span> between redundant computation and memory accesses. If we make more synchronizations for the intermediate layers, more memory accesses will occur because we have to store the intermediate results and load these results later. However, if we make less synchronizations, we will probably do much redundant computation works. Therefore, a better trade-off between redundant computation and memory accesses has to be made in order to reduce latency. This trade-off is decided by how we schedule, such as split, reorder and fuse our loops. For example, the size of tiles are being heavily tested in our implementation. For example, a major drop takes place if the sizes of our tile in convolutional layers grow to be larger than the size of cache.
</p><p class="parag-418">
In our CNN implementations, <span class="imp2">synchronization</span> is made by "compute_root() or compute_at()". The intermediate results for a Func will be stored only when we schedule this Func with compute_root(). We basically set compute_root() for the Funcs that compute each output pixels with multiple pixels and parameters. But a more careful tuning here may help further increase the speed. At first we performed no compute_root on every layer, which results in a full inline version of CNN, leading to a performance of more than 200x slower than Caffe. However, it is not necessary to put compute_at() in every layer or put compute_at() at many dimensions. Doing too much stage synchronization would hurt the performance significantly as well. For example, applying compute_root() at every layer would lead to a 2~5x slower version of our code. We need to try to test more on this part in order to improve the performance.
</p><p class="parag-418">
Within each Func, we paralleled with <span class="imp2">multi-threading and SIMD</span> on different levels. We put the multi-threading at the outer loop to decrease the proportion of the scheduling overhead. And we use SIMD in the innermost loop of cnnff to reduce divergence, as well as make better use of locality. Also, we split the maps and reorder the for loops, to further deploy the locality. The implementation does not lack parallelism, but there might be some potential to better schedule the computation to fit the computating and memory resources of different machines and settings. The choice of levels to perform parallelism influences the performance to a great degree. For example, if we change the vectorization from x to higher or lower in the convolutional layer, we would notice a significant drop of performance as much as 3x. If we fuse z_outer and w in convolutional layer to do multi-threading, it does not change the performance but if we reorder the loops, such as reorder z_outer to be the outermost, the performance is hurt badly. This choice might limit our performance because we might not choose the best order and level of parallelism up to this point. 
</p><p class="parag-418">
The different <span class="imp2">setups of CNNs</span> has some influence on the performance of our implementation as well. For example, different sizes of convolution kernel would affect the performance of our choice of parallelism. If one dimension of the kernel size is bigger than the number of SIMD lanes, it might be better to do vectorization on the domain variables (dom.x and dom.y) rather than x. There is nothing like a best set of parameters and schedule for every network, but it is possible to find a generally better solution to some frequently used CNNs as LeNet and AlexNet. We need to admit that the variety of networks pose a difficulty and limitation on our elevation of performance.
</p>

<h5>Room of Elevation</h5>
<p class="parag-418">
As we can tell from the two tables of profiling Caffe and Bromide's runtime on AlexNet with a batchsize 64, we significantly outperformed Caffe on convolutional layers but did bad on <span class="imp2">fully-connected layers</span>. This is because we did not manage to get a nicely tuned scheduling of this layer up to now. Although in fully-connected layers, if using matrix multiplication, we no longer need to do so much copying and redundant work as in convolutional layers, which can better take the advantage of fine-tuned matrix multiplication, we still think it might be feasible to get the same or even better performance on this layer than Caffe if we can tune the schedule better and given that Halide provides a lot of optimization under the scheduling. Elevation of the performance of Bromide on fully-connected layers to the same level of Caffe would enable Bromide to have a much better general performance.
</p>
<p class="parag-418">Besides, the tuning of convolutional layers is not perfect. Using the current strategy, we think there is still a lot of space to explore, for example we could go with different tile size with different order of loops. Tuning these parameters towards the specification of the machine would further bring up the performance.</p>

<h5>Future Work</h5>
<div class="parag-418">
There are a few ways to approach the convolution operation, we did not test the last two methods because we deem that they would not outperform the first one in our scenarios after our analysis, but for completeness, we would like to continue to compare our current implementaton with them:
<ul class="parag-418-in">
<br/>
<li>Direct convolution with Halide scheduling.</li>
<li>Converting convolution to matrix multiplication, and scheduling with Halide. </li>
<li>Converting convolution to dot product by FFT, and scheduling with Halide.</li>
</ul>
</div>

<p class="parag-418">
Our future work will also include allowing the program to adaptively select a close to best scheduling strategy for each network and machine type. Currently, we have to differentiate the details of scheduling for LeNet and AlexNet, such as tiling and reordering. However, it would be a great step if the scheduling stategy can be automatically created for each application.
</p>

<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>References</h4>
<div class="parag-418">
<ul class="parag-418">
<li>Halide source code: <a href="https://github.com/halide/Halide">https://github.com/halide/Halide. </a></li>
<li>Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F. and Amarasinghe, S., 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. ACM SIGPLAN Notices, 48(6), pp.519-530. </li>
<li>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.</li>
<li>LeCun, Yann, et al. "Comparison of learning algorithms for handwritten digit recognition." International conference on artificial neural networks. Vol. 60. 1995.</li>
</ul>
</div>


<!--&&&&&& divider &&&&&&-->
<div class="par-blankline-10"></div>
<h4>Work Divided</h4>
<p class="parag-418">
Roughly equal work by each student.
</p>



<!--&&&&&& divider &&&&&&-->
            </div>

        </div>

    </div>
</div>
