---
layout: default
encoding: "utf-8"
---


<div class="container-fluid index">
    <div class="row">

          <div class="col-md-12 main content-panel">

            <div class="articles">

              <h2>15-418 Final Project Report</h2>

              <h3>Bromide -- Fast CNN Inference In Halide</h3>
              <br/>
              <a class="links-sm-418" href="{{ site.baseurl }}/15418">back to project index</a>
              <br/>
              <br/>



<h4>Summary</h4>
<p class="parag-418">
We have implemented a general <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> inference program using <a href="http://halide-lang.org/">the Halide image processing language</a> which is tuned to be fast(-er than Caffe) on a single machine using CPU.
</p>

<h4>Background</h4>
<br/>
<h5>CNN</h5>
<p class="parag-418">
Lots of modern-day machine learning applications, such as AlphaGo, are using Convolutional Neural Networks (CNNs) for classification tasks. Therefore, the performance of training and inference with CNNs is fairly important. For huge data sets and large CNN configurations, training can take several days, making it meaningful to speed up the training process. Besides, in real-time applications, such as Apple Siri, the time spent in inference is also expected to be short, so that the delay will be tolerable or more satisfactory for users.
</p>

<p class="parag-418">For testing purpose, we are using <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> which is designed to do classification towards <a href="http://image-net.org/">ImageNet</a>. The structure of AlexNet is illustrated as bewlow:</p>

<img src="{{ site.baseurl }}/resources/alexnet.png" alt="alexnet" height="312" width="600" /> 


<br/><br/>

<h5>Halide</h5>
<p class="parag-418">
Halide is a language and compiler for optimizing parallelism, locality, and re-computation in image processing pipelines. Since it provides many scheduling interfaces to handle matrices, it is a great source for CNN implementation, where the intermediate results are also matrices. Therefore, by carefully scheduling CNN training and inference with Halide, an obvious speedup is expected.
</p>

<h4>Methodology</h4>
<p class="parag-418">
We would introduce the details of the implementation (majorly scheduling) by each layer. Input layer, flatten layer, activation layer and drop layer are fairly straightforward thus we would mainly discuss other layers about their scheduling.
</p>

<p class="parag-418">
There are mainly two versions of our code, and, intuitively, version 2 is the more tuned version. The following discussion will include the comparison of the two versions.
</p>

<h5>Convolutional Layer</h5>
<p class="parag-418">
Version 1:<br/>
We are mainly doing intuitive implementation as well as using Halide::sum() and compute_root() in version 1. In convolutional layer, each pixel of an output map is computed according to the definition as the sum of weight of a filter window. 
</p>

<h5>Fully Connected Layer</h5>
<p class="parag-418">
</p>

<h5>Pooling Layer</h5>
<p class="parag-418">
</p>

<h5>Loss Layer</h5>
<p class="parag-418">
</p>

<h4>Challenge</h4>
<p class="parag-418">
Halide is a language targeted at image processing, which provides a set of computation scheduling methods to help programmers exploit data locality and achieve higher performance when writing parallel programs. However, since it is not designed for deep learning methods, it does not have intrinsic support for the typical computation patterns or algorithms in deep learning, for example, convolution.  Nowadays there are many a mature (sort of) implementation of deep learning methods. <br/><br/>For example, Caffe, is a frequently seen framework in the world of deep neural networks. One feature about Caffe is that it takes advantage of a fast matrix multiplication library like Intel's MKL when running convolutional neural networks by using matrix multiplications. However, breaking down the convolutional layers to matrix multiplications might not produce a desirable performance speedup since Halide does not natively support matrix multiplication. We think using FFT could avoid both the original complex convolution computations and the need for fast matrix multiplication. Although doing the DFT and its inverse will take some time but this might be better than the matrix multiplications way. So the challenge here is how to schedule and tune the program using Halide to achieve admissible performance on CPU or/and GPU, compared to the current matrix multiplication approach.  Another challenge would be to implement a set of complex modern deep neural networks using Halide. Halide, as a image processing language, is likely to have some constraints or limitation in the elevation of performance in the deep learning algorithms. It might be a challenge to use its scheduling methods to achieve desirable speedup.
</p>


<h4>Resources</h4>
<div class="parag-418">
<ul class="parag-418">
<li>Halide source code: <a href="https://github.com/halide/Halide">https://github.com/halide/Halide. </a></li>
<li>Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F. and Amarasinghe, S., 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. ACM SIGPLAN Notices, 48(6), pp.519-530. </li>
<li>(Can we get to learn about the automatic scheduling Halide program from Ravi Mullapudi?)</li>
</ul>
</div>

<h4>Goals and Deliverables</h4>

<div class="parag-418">
<h5>Plan to achieve:</h5>
<p class="parag-418">
High performance inferring on CNN (native CPU and GPU). It includes convolution, pooling, normalization, activation, and fully-connected layers. We will try improving the performance of forwarding these layers with Halide. </p>
<p class="parag-418">
High performance training on CNN (native CPU and GPU). It includes computing the derivatives of parameters in convolution, pooling, normalization, activation, and fully-connected layers. Again, we will use Halide to increase the performance. </p>
<p class="parag-418">
To demonstrate, CNN inferring consumes most time on the convolution layers. So our focus will be on improving the performance of this layer. Three methods can be used: 
</p>
<ul class="parag-418">
<li>Direct convolution with Halide scheduling.</li>
<li>Converting convolution to matrix multiplication, and scheduling with Halide. </li>
<li>Converting convolution to dot product by FFT, and scheduling with Halide.</li>
</ul>


<h5>Hope to achieve:</h5>
<p class="parag-418">
Implementing a set of modern deep convolutional networks based on the classic implementations of inferring and training on CNN
Analyzing the advantages and disadvantages of Halide, both in a general way and by comparing to Caffe (and some other implementations), to see what can be done to improve it (for example the support for Xeon Phi).
</p>

<h5>Demo:</h5>
<p class="parag-418">
We would like to show a graphical interpretation of our scheduling methods for the networks we are implementing, as well as the corresponding results of them compared to naive (according to the original algorithms, without too much organization, but somewhat optimized) C++ code and Caffe (if applicable). The results would be composed of running time given input data and computation resources used.
</p>
</div>

<h4>Platform</h4>
<div class="parag-418">
<ul class="parag-418">
<li>GHC Machines: ghc[47-84].ghc.andrew.cmu.edu (Intel® Core™ i7-4785T Processor, 4 cores, 8 threads, SSE4.1/4.2, AVX 2.0)</li>
<li>ghc41.ghc.andrew.cmu.edu (CUDA cores: 2304)</li>
</ul>
</div>

<h4>Schedule</h4>
<div class="parag-418">
<h5>4/1 - 4/8: </h5>

Get familiar with Halide, by studying the toturials;<br/>
Implement the inference process of CNN with Halide: including convolution layers with scheduling and FFT-based approaches, and pooling, normalization, activation and fully connected layers.

<h5>
4/9 - 4/15:</h5>
Implement training process of CNN with Halide;<br/>
Trying using the automatic scheduling tool by Ravi on our program.
<h5>
4/16 - 4/22:</h5>
Select some modern deep neural networks and implement them with Halide. 
<h5>
4/23 - 4/29:</h5>
Compare performance with Caffe on CPU and GPU platforms, in terms of training and inference.
</div>           

            </div>

        </div>

    </div>
</div>
