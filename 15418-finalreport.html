---
layout: default
encoding: "utf-8"
---


<div class="container-fluid index">
    <div class="row">

          <div class="col-md-12 main content-panel">

            <div class="articles">

              <h2>15-418 Final Project Report</h2>

              <h3>Bromide -- Fast CNN Inference In Halide</h3>
              <br/>
              <a class="links-sm-418" href="{{ site.baseurl }}/15418">back to project index</a>
              <br/>
              <br/>



<h4>Summary</h4>
<p class="parag-418">
We have implemented a general <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> inference program using <a href="http://halide-lang.org/">the Halide image processing language</a> which is scheduled to be <span class="imp">fast(-er than Caffe, up to 3x)</span> on a single machine using CPU.
</p>

<div class="par-blankline-10"></div>
<h4>Background</h4>
<br/>
<h5><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a></h5>
<p class="parag-418">
Lots of modern-day machine learning applications, such as AlphaGo, are using Convolutional Neural Networks (CNNs) for classification tasks. Therefore, the performance of training and inference with CNNs is fairly important. For huge data sets and large CNN configurations, training can take several days, making it meaningful to speed up the training process. Besides, in real-time applications, such as Apple Siri, the time spent in inference is also expected to be short, so that the delay will be tolerable or more satisfactory for users.
</p>

<p class="parag-418">For testing purpose, we are using <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> (a database of handwritten digits and we use a typical CNN, <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a> to perform inference on it) and <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>(which is designed to do classification towards <a href="http://image-net.org/">ImageNet</a>). 

<p class="parag-418">
The structure of LeNet is illustrated as below:</p>
<img src="{{ site.baseurl }}/resources/lenet.png" alt="lenet" height="212" width="600" /> 

<br/><br/>

<p class="parag-418">
The structure of AlexNet is illustrated as below:</p>

<img src="{{ site.baseurl }}/resources/alexnet.png" alt="alexnet" height="312" width="600" /> 

<br/><br/>

<h5><a href="http://halide-lang.org/">Halide</a></h5>
<p class="parag-418">
Halide is a language and compiler for optimizing parallelism,locality, and re-computation in image processing pipelines. Since it provides many scheduling interfaces to handle matrices, it is a great source for CNN implementation, where the intermediate results are also matrices. Therefore, by carefully scheduling CNN training and inference with Halide, an obvious speedup is expected.
</p>

<div class="par-blankline-10"></div>
<h4>What we did (Results)</h4>
<p class="parag-418">
We have implemented this inference program which is tested towards MNIST (LeNet) and <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet"> AlexNet (ImageNet)</a>, and it is compared against <a href="https://github.com/BVLC/caffe">Caffe</a> (testing of the network). Luckily, we have achieved better performance than Caffe in our limited tested scenarios. All of these tests are conducted on a machine that has 2 processors (Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz). The following figures and data generally use Caffe as their counterpart for comparison. Our implementation is referred to as <span class="brom">"bromide"</span>.
</p>

<p class="parag-418">
The <span class="imp">throughput</span> of LeNet is illustrated as below:</p>
<img src="{{ site.baseurl }}/resources/lenet_throughput.jpg" alt="lenet" height="362" width="600" /> 
<br/><br/>

<p class="parag-418">
The <span class="imp">latency</span> of LeNet and AlexNet is illustrated as below: (<span class="imp">latency</span> here stands for the time to perform inference on a single input image)
</p>
<img src="{{ site.baseurl }}/resources/latency.jpg" alt="latency" height="352" width="600" /> 
<br/><br/>

<p class="parag-418">
As we can see from the figures above, for the batchsize 16 of LeNet, the speedup (in terms of throughput) of Bromide over Caffe is roughly <span class="imp">3.04x</span>, and the average latency is <span class="imp">21% (LeNet) and 26% (AlexNet) better</span> than Caffe. Although these are not very significant, the results surely give us some hope in the power of scheduling in Halide.
</p>

<div class="par-blankline-10"></div>
<h4>How we did it (Methodology)</h4>
<p class="parag-418">
We would introduce the details of the implementation (majorly scheduling) by each layer. Flatten layer, activation layer and drop layer are fairly straightforward thus we would mainly discuss other layers about their scheduling.
</p>
<p class="parag-418">
There are mainly two versions of our code, and, intuitively, version 2 is the more tuned version. The following discussion will include the comparison of the two versions.
</p>

<h5>Basic Settings</h5>
<p class="parag-418">
Every layer has a member called <span class="imp2">cnnff</span>, standing for the collection of the feedforward values. It generally has 4 dimensions, <span class="imp2">(x, y, z, w)</span>, corresponding to the column-index, row-index, map-index (3rd dimension), and batch size. All those layers are written in separate files and they are welded in the main function to build networks for testing. Scheduling is performed inside the layers.
</p>


<h5>Input Layer</h5>
<p class="parag-418">
We process the input data (trained weights) using a similar way to Caffe. The model (bvlc_alexnet.caffemodel) and mean (imagenet_mean.inarayproto) are parsed based on the functions provided by Caffe responsible for translating files using Protocol Buffer.
</p>

<h5>Convolutional Layer</h5>
<p class="parag-418">
Version 1:
</p>
<blockquote class="parag-418">conv_basic(x, y, z, w) = Halide::sum(last_layer.cnnff(x + dom.x, y + dom.y, dom.z + group_idx * input_group_size, w) * kernels(dom.x, dom.y, dom.z, map_idx + group_idx * group_size) + bias(z));</blockquote>
<p class="parag-418">
We are mainly doing intuitive implementation as well as using <span class="imp2">Halide::sum()</span> and <span class="imp2">Func.ompute_root()</span> in version 1. In convolutional layer, each pixel of an output map is computed according to the definition as the weighted sum of a filter window.
</p>
<p class="parag-418">
The <span class="imp2">Halide::sum()</span> function is used for inline reduction over the given domain (in this case, the domain would be 3-d, corresponding to the filter window of convolution). Func.compute_root() in Halide is used to compute the <span class="imp2">Func</span> ahead of the following program (<span class="imp2">Funcs</span> will be combined inline in Halide if there is no functions like compute_root() or compute_at() to explicitly order it to perform computation to the point). Func is a Halide class standing for a pipleline stage, which is a pure function that defines what value each pixel should have, similar to a computed image.
</p>

<p class="parag-418">
Version 2:<br/>
</p>
<blockquote class="parag-418">
  
input_ = Halide::BoundaryConditions::constant_exterior(last_layer.cnnff, 0, 0, last_layer.size[0], 0, last_layer.size[1]);<br/>
cnnff(x, y, z, w) = bias(z);<br/>
cnnff(x, y, z, w) += kernels(r.x, r.y, r.z % input_group_size, z) * input_(x * stride_x + r.x - pad_x, y * stride_y + r.y - pad_y, r.z, w);<br/>
cnnff.update().split(y, y_o, y_i, 32).split(z, z_o, z_i, 32); <br/>
cnnff.update().reorder(y_i, z_i, r.z, y_o, z_o).vectorize(x, 8); <br/>
cnnff.update().parallel(w);<br/>
cnnff.update().unroll(r.x).unroll(r.y);<br/>
input_.compute_at(cnnff, z_i);<br/>

</blockquote>
<p class="parag-418">
Convolution layer is a meaningful implementation exploiting the power of scheduling in Halide. We basically split y and z and reorder to get a tile, in order to find a point that reaches a good balance of memory traffic and locality which leads to a good performance. A typical tile would be loop over (from inner to outer) y_inner, y_outer, z_inner, z_outer, however, reordering the domain loop r.z before y_outer or z_outer would led to a elevation of performance because of more locality. Vectorization, parallelism and unrolling were performed here as well. It is fairly intuitive to vectorize over the innermost variable and parallelize the outermost one. Unrolling is performed at two innermost domain variables since they have a typically small range. The <span class="imp2">compute_at(func, var)</span> function performs computation of func for every unique value of var, which would enable us to compute the block of inner loop variables (y_i and z_i) to actually instantiate the above scheduling.
</p>

<h5>Fully Connected Layer</h5>
<blockquote class="parag-418">
Halide::RDom dom(0, last_layer.size[0]);<br/>
      cnnff(x, y, z, w) = Halide::sum(last_layer.cnnff(dom.x, y, z, w) * weights(dom.x, x)) + bias(x); <br/>
      cnnff.parallel(w);<br/>
          cnnff.vectorize(x, 8);<br/>
          cnnff.compute_root();<br/>
</blockquote>

<p class="parag-418">
Fully connected layer is basically a <span class="imp2">matrix multiplication</span>. If the batch size is equal to 1, it's a matrix multiplying vector. Otherwise, it's a multiplication between two matrices. Therefore, all the optimization methods can be used here. Note that for the multiplication between a matrix and a vector, there's data reuse only in terms of the vector, but no data reuse for matrix. So a reasonable scheduling is row-by-row multiplication between matrix and vector. However, for the multiplication between two matrices, more sophisticated methods should be considered. In this project, we tried tiling the matrices into blocks, and tuning the block size to let it fit in the L3 cache. But we found that in our experiments, similarly good perforamnce can be achieved by the straighforward impelmentation of matrix multiplication. 
</p>
<p class="parag-418">
Also, for both matrix-vector multiplication and matrix-matrix multiplication, we make use of SIMD and multi-threading implementation. We schedule them in different dimensions so that they can work together with less synchronization. We use multi-threading for the "batch" dimension, referring to any test image. And we use SIMD for each image. To compute each pixel, there's a inner-product between vectors. This procedure is done by the reduction function <span class="imp">sum()</span> built in Halide.
</p>

<h5>Pooling Layer</h5>
<blockquote class="parag-418">
Halide::Func sub_maps("sub_maps");<br/>
                Halide::RDom dom(0, pool_x, 0, pool_y);<br/>
                if(pool_func == "max") {<br/>
                &nbsp;&nbsp;        sub_maps(x, y, z, w) = Halide::maximum(last_layer.cnnff(x + dom.x, y + dom.y, z, w));<br/>
                }else if(pool_func == "average") {<br/>
                &nbsp;&nbsp;        sub_maps(x, y, z, w) = Halide::sum(last_layer.cnnff(x + dom.x, y + dom.y, z, w)) / (pool_x * pool_y);<br/>
                }<br/>
                cnnff(x, y, z, w) = sub_maps(x * stride_x - pad_x, y * stride_y - pad_y, z, w);<br/>
        cnnff.vectorize(x, 8);<br/>
                cnnff.compute_root();<br/>

</blockquote>

<p class="parag-418">
Pooling layer is similar to the convolution layer, in that there's a window moving above the maps, and doing a reduction at each time. Therefore, similar optimization can be used in pooling.
</p>
<p class="parag-418">
But we also notice that, pooling doesn't take a large proportion in the overall latency. Therefore, we simply used the built-in reduction function max, and vectorized it by SIMD. We didn't choose to use multi-threading, because the overhead is not worthwhile. In our observation, pooling layer of bromide can achieve 10x speedup compared to caffe implementation.
</p>

<div class="par-blankline-10"></div>
<h4>Challenges met and TO-DOs</h4>
<p class="parag-418">
Halide is a language targeted at image processing, which provides a set of computation scheduling methods to help programmers exploit data locality and achieve higher performance when writing parallel programs. However, since it is not designed for deep learning methods, it does not have intrinsic support for the typical computation patterns or algorithms in deep learning, for example, convolution.  Nowadays there are many a mature (sort of) implementation of deep learning methods. <br/><br/>For example, Caffe, is a frequently seen framework in the world of deep neural networks. One feature about Caffe is that it takes advantage of a fast matrix multiplication library like Intel's MKL when running convolutional neural networks by using matrix multiplications. However, breaking down the convolutional layers to matrix multiplications might not produce a desirable performance speedup since Halide does not natively support matrix multiplication. We think using FFT could avoid both the original complex convolution computations and the need for fast matrix multiplication. Although doing the DFT and its inverse will take some time but this might be better than the matrix multiplications way. So the challenge here is how to schedule and tune the program using Halide to achieve admissible performance on CPU or/and GPU, compared to the current matrix multiplication approach.  Another challenge would be to implement a set of complex modern deep neural networks using Halide. Halide, as a image processing language, is likely to have some constraints or limitation in the elevation of performance in the deep learning algorithms. It might be a challenge to use its scheduling methods to achieve desirable speedup.
</p>

<div class="parag-418">
There are many ways to approach the convolution operation.
<ul class="parag-418">
<li>Direct convolution with Halide scheduling.</li>
<li>Converting convolution to matrix multiplication, and scheduling with Halide. </li>
<li>Converting convolution to dot product by FFT, and scheduling with Halide.</li>
</ul>
</div>



<h4>Resources</h4>
<div class="parag-418">
<ul class="parag-418">
<li>Halide source code: <a href="https://github.com/halide/Halide">https://github.com/halide/Halide. </a></li>
<li>Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F. and Amarasinghe, S., 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. ACM SIGPLAN Notices, 48(6), pp.519-530. </li>
<li>(Can we get to learn about the automatic scheduling Halide program from Ravi Mullapudi?)</li>
</ul>
</div>

<h4>Goals and Deliverables</h4>

<div class="parag-418">
<h5>Plan to achieve:</h5>
<p class="parag-418">
High performance inferring on CNN (native CPU and GPU). It includes convolution, pooling, normalization, activation, and fully-connected layers. We will try improving the performance of forwarding these layers with Halide. </p>
<p class="parag-418">
High performance training on CNN (native CPU and GPU). It includes computing the derivatives of parameters in convolution, pooling, normalization, activation, and fully-connected layers. Again, we will use Halide to increase the performance. </p>
<p class="parag-418">
To demonstrate, CNN inferring consumes most time on the convolution layers. So our focus will be on improving the performance of this layer. Three methods can be used: 
</p>
<ul class="parag-418">
<li>Direct convolution with Halide scheduling.</li>
<li>Converting convolution to matrix multiplication, and scheduling with Halide. </li>
<li>Converting convolution to dot product by FFT, and scheduling with Halide.</li>
</ul>



<h5>Demo:</h5>
<p class="parag-418">
We would like to show a graphical interpretation of our scheduling methods for the networks we are implementing, as well as the corresponding results of them compared to naive (according to the original algorithms, without too much organization, but somewhat optimized) C++ code and Caffe (if applicable). The results would be composed of running time given input data and computation resources used.
</p>
</div>

          

            </div>

        </div>

    </div>
</div>
